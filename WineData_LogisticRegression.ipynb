{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng(1234567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a pandas dataframe\n",
    "wine_data = pd.read_csv('./wine-classification-missing.csv', sep=';', index_col=0)\n",
    "\n",
    "# View 10 random rows of the data\n",
    "train = wine_data.sample(10)\n",
    "\n",
    "# Convert the DataFrame to a NumPy array for shuffling\n",
    "wine_data_array = wine_data.to_numpy()\n",
    "\n",
    "# Shuffle the rows using rng.shuffle\n",
    "rng.shuffle(wine_data_array)\n",
    "\n",
    "# Convert the shuffled array back to a DataFrame\n",
    "wine_data_rng = pd.DataFrame(wine_data_array, columns=wine_data.columns)\n",
    "\n",
    "#Split data set\n",
    "train_set_value = round(0.7 * len(wine_data_rng))\n",
    "val_set_value = round(0.15 * len(wine_data_rng))\n",
    "test_set_value = round(0.15 * len(wine_data_rng))\n",
    "\n",
    "\n",
    "#Take the first 70% of data from dataset\n",
    "wine_data_train_set = wine_data_rng.iloc[:train_set_value]\n",
    "#Take 15%\n",
    "wine_data_val_set = wine_data_rng.iloc[train_set_value: train_set_value + val_set_value]\n",
    "#Store the rest\n",
    "wine_data_test_set = wine_data_rng.iloc[(train_set_value)+ val_set_value:]\n",
    "\n",
    "#Set training sample\n",
    "train_set_sample = wine_data_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alcohol                  13.005920\n",
      "malic_acid                2.350660\n",
      "ash                       2.388960\n",
      "alcalinity_ash           19.742400\n",
      "magnesium               100.144000\n",
      "total_phenols             2.264400\n",
      "flavanoids                1.998320\n",
      "nonflavanoid_phenols      0.363040\n",
      "proanthocyanins           1.573200\n",
      "color_intensity           4.991920\n",
      "hue                       0.950519\n",
      "OD280_OD315               2.602547\n",
      "proline                 749.408000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values with means and standardize the data\n",
    "def filter_data(data):\n",
    "    #Stores all collumns with Nan values\n",
    "    features_with_nan = data.columns[data.isna().any()]\n",
    "    \n",
    "    #Finds mean of each column that has NaN values\n",
    "    features_means = data[features_with_nan].mean()\n",
    "    \n",
    "    #Fill the NaN values with the means for each specific column\n",
    "    data_cleaned = data.fillna(features_means)\n",
    "    \n",
    "    #Store the data class for later use\n",
    "    data_label = data_cleaned['class']\n",
    "    \n",
    "    #Drop class for calculations\n",
    "    data_without_class = data_cleaned.drop('class', axis=1)\n",
    "    \n",
    "    #Find the mean and standard deviation and store it\n",
    "    mean_values = data_without_class.mean()\n",
    "    std_values = data_without_class.std()\n",
    "    \n",
    "    #Standardization for each feature\n",
    "    standardized_data = (data_without_class - mean_values) / std_values\n",
    "    \n",
    "    return standardized_data, data_label, mean_values, std_values\n",
    "\n",
    "train_set_standardized, train_set_label, mean_train_set, std_train_set = filter_data(wine_data_train_set)\n",
    "print(mean_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.992\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set ytrain to the class feature\n",
    "yTrain = train_set_label\n",
    "#set xtrain to all features beside class\n",
    "XTrain = train_set_standardized\n",
    "# Add a column of ones called bias to the DataFrame, making it now a design matrix\n",
    "XTrain['bias'] = 1\n",
    "\n",
    "# Get the number of training examples\n",
    "samples_len = len(yTrain)\n",
    "\n",
    "\n",
    "#Compute the cost function for logistic regression with L2 regularization.\n",
    "def compute_cost(targets, design_matrix, lambda_val, params):\n",
    "    \n",
    "    # Calculate the predicted probabilities using the logistic function\n",
    "    predicted_prob = logistic(np.dot(design_matrix, params))\n",
    "    \n",
    "    # Calculate the L2 regularization to penalize the large parameter values\n",
    "    l2_regression = (lambda_val / (2*samples_len)) * np.sum(params[1:]**2)\n",
    "    \n",
    "    # Calculate the logistic regression cost function with L2 regularization\n",
    "    cost = (-1 / samples_len) * (np.dot(targets.T, np.log(predicted_prob)) + \n",
    "                       np.dot((1 - targets).T, np.log(1 - predicted_prob))) + l2_regression\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(targets, design_matrix, alpha, lambda_val, params, iterations):\n",
    "    \n",
    "    #create empty array to use later\n",
    "    costs = []\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        # calculate the predicted probabilities\n",
    "        predicted_prob = logistic(np.dot(design_matrix, params))\n",
    "        \n",
    "        #calculate the gradient\n",
    "        gradient = (1 / samples_len) * np.dot(design_matrix.T, (predicted_prob - targets))\n",
    "        \n",
    "        #Apply the L2 regularization to the gradient (not including the bias)\n",
    "        gradient[1:] += (lambda_val / samples_len) * params[1:]\n",
    "        \n",
    "        #Update the parameters\n",
    "        params -= alpha * gradient\n",
    "        \n",
    "        #calculate and store the costs for monitoring the convergence\n",
    "        cost = compute_cost(targets, design_matrix, lambda_val, params)\n",
    "        \n",
    "        #Add to list\n",
    "        costs.append(cost)\n",
    "        \n",
    "    return params, costs\n",
    "\n",
    "\n",
    "#Set Variables\n",
    "# Initialize parameters to zeros\n",
    "initial_params = np.zeros(XTrain.shape[1])\n",
    "\n",
    "# Set the learning rate\n",
    "alpha = 0.01\n",
    "\n",
    "# Set the regularization parameter\n",
    "lambda_val = 0.1\n",
    "\n",
    "params, costs = gradient_descent(yTrain, XTrain, alpha, lambda_val, initial_params, iterations=1000)\n",
    "\n",
    "# Use the final parameters to make predictions\n",
    "predicted_classes = (logistic(np.dot(XTrain, params)) >= 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_classes == yTrain)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Plot the cost over iterations\n",
    "plt.plot(costs)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the val set\n",
    "val_set_standardized, val_set_label, mean_val_set, std_val_set = filter_data(wine_data_val_set)\n",
    "\n",
    "#Set ytrain to the class feature\n",
    "yVal = val_set_label\n",
    "\n",
    "#set xtrain to all features beside class\n",
    "XVal = val_set_standardized\n",
    "\n",
    "# Add a column of ones called bias to the DataFrame, making it now a design matrix\n",
    "XVal['bias'] = 1\n",
    "\n",
    "# Get the number of training examples\n",
    "samples_val_len = len(yVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid function\n",
    "def logistic(z):\n",
    "    return 1 / (1 + np.exp(-z)) #1/1+e-z\n",
    "\n",
    "\n",
    "# measures the difference between the predicted probabilities and the true labels\n",
    "def bce_loss(X, y, w, gamma):\n",
    "    \n",
    "    # Calculate the predicted probabilities using the logistic function\n",
    "    predicted_prob = logistic(np.dot(X, w))\n",
    "    \n",
    "    # Calculate the L2 regularization to penalize the large parameter values for bce\n",
    "    l2_regression = (gamma / (2*len(y))) * np.sum(w[1:]**2)\n",
    "    \n",
    "    # Calculate the binary cross-entropy loss with L2 regularization\n",
    "    loss = (1 / len(y)) * (np.dot(y.T, np.log(predicted_prob)) + \n",
    "                       np.dot((1 - y).T, np.log(1 - predicted_prob))) + l2_regression\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def mgd_optimizer(X, y, gamma, eta, S, max_iters):\n",
    "\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    \n",
    "    for iteration in range(max_iters):\n",
    "        # Select random minibatch of size S\n",
    "        rand_batch = np.random.choice(len(y), S, replace=False)\n",
    "        X_batch = X.iloc[rand_batch]\n",
    "        y_batch = y.iloc[rand_batch]\n",
    "        \n",
    "        #Calculate the preicted probabilities\n",
    "        predicted_prob = logistic(np.dot(X_batch, weights))\n",
    "        \n",
    "        #calculate gradient\n",
    "        gradient = (1 / S) * np.dot(X_batch.T, (predicted_prob - y_batch))\n",
    "        \n",
    "        #Applt the l2 reg to the gradient, not including bias\n",
    "        gradient[1:] += (gamma / len(y)) * weights[1:]\n",
    "        \n",
    "        #Update weights\n",
    "        weights -= eta * gradient\n",
    "        \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma: 0.004641588833612777\n",
      "Best eta: 1e-05\n",
      "Best loss: -0.6924884329185288\n",
      "Best S: 1\n"
     ]
    }
   ],
   "source": [
    "# Grid of values for gamma, eta, and S\n",
    "num = 10\n",
    "gamma_vector = np.logspace(-4, -1, num)\n",
    "eta_vector = np.logspace(-5, -1, num)\n",
    "S_vector = np.linspace(1, 15, num)\n",
    "best_loss = float('inf')\n",
    "\n",
    "for gamma in gamma_vector:\n",
    "    for eta in eta_vector:\n",
    "        for S in S_vector:\n",
    "            # Use mgd_optimizer to compute weights with given gamma, eta, and S\n",
    "            weights = mgd_optimizer(XTrain, yTrain, gamma, eta, int(S), max_iters=150)\n",
    "\n",
    "            # Use the computed weights to calculate the loss on the validation set\n",
    "            validation_loss = bce_loss(XVal, yVal, weights, gamma)\n",
    "\n",
    "            # Check if the current validation loss is the smallest so far\n",
    "            if validation_loss < best_loss:\n",
    "                best_loss = validation_loss\n",
    "                best_weights = weights\n",
    "                best_gamma = gamma\n",
    "                best_eta = eta\n",
    "                best_S = int(S)\n",
    "\n",
    "\n",
    "# Print and save the best values\n",
    "print(\"Best gamma:\", best_gamma)\n",
    "print(\"Best eta:\", best_eta)\n",
    "print(\"Best loss:\", best_loss)\n",
    "print(\"Best S:\", best_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the validation and test set and then filter it\n",
    "combined_wine_data = pd.concat([wine_data_val_set, wine_data_test_set], axis=0, ignore_index=True)\n",
    "combined_set_standardized, combined_set_label, mean_combined_set, std_combined_set = filter_data(combined_wine_data)\n",
    "\n",
    "#Set ytrain to the class feature\n",
    "y_combined = combined_set_label\n",
    "\n",
    "#set xtrain to all features beside class\n",
    "X_combined = combined_set_standardized\n",
    "\n",
    "# Add a column of ones called bias to the DataFrame, making it now a design matrix\n",
    "X_combined['bias'] = 1\n",
    "\n",
    "# Use mgd_optimizer to compute weights with given gamma, eta, and S\n",
    "weights = mgd_optimizer(X_combined, y_combined, best_gamma, best_eta, best_S, max_iters=150)\n",
    "\n",
    "# Use the computed weights to calculate the loss\n",
    "combined_loss = bce_loss(X_combined, y_combined, best_weights, best_gamma)\n",
    "\n",
    "\n",
    "# Preprocess the test data using the saved mean and std values\n",
    "test_set_cleaned = wine_data_test_set.fillna(mean_combined_set)\n",
    "test_set_standardized = (test_set_cleaned - mean_combined_set) / std_combined_set\n",
    "\n",
    "# Set y_test to the class feature\n",
    "y_test = test_set_cleaned['class']\n",
    "\n",
    "# Set x_test to all features beside class\n",
    "X_test = test_set_standardized.drop('class', axis=1)\n",
    "\n",
    "# Add a column of ones called bias to the DataFrame, making it a design matrix\n",
    "X_test['bias'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Binary Cross-Entropy Loss: -0.6925113873609788\n",
      "Test Accuracy: 0.8461538461538461\n",
      "Test Precision: 0.7272727272727273\n",
      "Test Recall: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# Reporting Test Metrics #\n",
    "\n",
    "# Use mgd_optimizer to compute weights with the optimal gamma, eta, and S on the test set\n",
    "weights_test = mgd_optimizer(X_test, y_test, best_gamma, best_eta, best_S, max_iters=150)\n",
    "\n",
    "# Use the computed weights to calculate the binary cross-entropy loss on the test set\n",
    "test_loss = bce_loss(X_test, y_test, weights_test, best_gamma)\n",
    "\n",
    "# Use the final parameters to make predictions on the test set\n",
    "predicted_classes_test = (logistic(np.dot(X_test, weights_test)) >= 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy_test = np.mean(predicted_classes_test == y_test)\n",
    "\n",
    "# Calculate precision and recall on the test set\n",
    "true_positives = np.sum((predicted_classes_test == 1) & (y_test == 1))\n",
    "false_positives = np.sum((predicted_classes_test == 1) & (y_test == 0))\n",
    "false_negatives = np.sum((predicted_classes_test == 0) & (y_test == 1))\n",
    "\n",
    "precision_test = true_positives / (true_positives + false_positives)\n",
    "recall_test = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "# Print the results\n",
    "print(\"Test Binary Cross-Entropy Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "print(\"Test Precision:\", precision_test)\n",
    "print(\"Test Recall:\", recall_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
